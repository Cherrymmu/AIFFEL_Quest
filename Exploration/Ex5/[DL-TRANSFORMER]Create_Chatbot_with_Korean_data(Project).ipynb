{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88867770",
   "metadata": {},
   "source": [
    "# PROJECT: 한국어 데이터로 챗봇 만들기\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 개요\n",
    "앞 선 예제에서 구현했던 '영어 데이터로 대화형 챗봇 만들기'를 참고하여 한국어 데이터를 사용하는 챗봇을 만들어본다.  \n",
    "\n",
    "## 목차\n",
    "1. 필요한 라이브러리 불러오기\n",
    "2. 데이터 수집하기\n",
    "3. 데이터 전처리하기\n",
    "4. SubwordTextEncoder 사용하기\n",
    "5. 모델 구성하기\n",
    "6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6dfba4",
   "metadata": {},
   "source": [
    "### STEP 1. 필요한 라이브러리 불러오기\n",
    "---\n",
    "프로젝트 진행에 필요한 라이브러리를 import하고, 주요 라이브러리의 버전을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985ab1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of Tensorflow: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "# 데이터 경로 접근을 위한 시스템 라이브러리\n",
    "import os\n",
    "# 연산을 위한 라이브러리\n",
    "import numpy as np\n",
    "# csv 파일 처리을 위한 라이브러리\n",
    "import pandas as pd\n",
    "# 데이터 전처리를 위한 라이브러리\n",
    "import re\n",
    "\n",
    "# 텐서 관리를 위한 라이브러리\n",
    "import tensorflow as tf\n",
    "# 단어장 생성을 위한 라이브러리\n",
    "import tensorflow_datasets as tfds\n",
    "# 모델 설계를 위한 라이브러리와 모듈들\n",
    "from tensorflow.keras import Input, Model, utils\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, LayerNormalization, Embedding, Lambda\n",
    "# 마스킹 처리를 위한 모듈\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# 손실함수 지정을 위한 모듈\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "# 학습률 지정을 위한 모듈\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "# 옵티마이저 지정을 위한 모듈\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# 평가지표 지정을 위한 모듈\n",
    "from tensorflow.keras.metrics import sparse_categorical_accuracy\n",
    "\n",
    "# 출력 결과 시각화를 위한 모듈\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Version of Tensorflow: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c9c92a",
   "metadata": {},
   "source": [
    "지금은 각 과정에서 어떤 라이브러리와 모듈을 사용하는지 파악하기 위해 한 번에 import를 했지만, 위와 같은 방법을 사용할 경우 향후 작성될 코드에서 어떤 라이브러리를 사용하는지 파악하기 어렵기 때문에 from ~ import는 지양하는 것이 좋다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135aeaba",
   "metadata": {},
   "source": [
    "### STEP 2. 데이터 수집하기\n",
    "---\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용한다.  \n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있다.  \n",
    "* [songys/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)  \n",
    "\n",
    "`Cloud shell`에서 아래 명령어를 입력하여 디렉터리를 생성하고 데이터를 추가한다.  \n",
    "```\n",
    "$ mkdir -p ~/aiffel/transformer_chatbot/data/\n",
    "$ cp ~/data/* ~/aiffel/transformer_chatbot/data/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7ce5f",
   "metadata": {},
   "source": [
    "### STEP 3. 데이터 전처리하기\n",
    "---\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만, 전체적으로는 다른 전처리를 수행해야 할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbf40fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/aiffel/aiffel/transformer_chatbot/data/ChatbotData .csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv 파일 경로 접근\n",
    "dirpath = os.getenv('HOME') + '/aiffel/transformer_chatbot/data'\n",
    "# csv 파일 접근\n",
    "filepath = os.path.join(dirpath, 'ChatbotData .csv')\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4092b42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "raw_data = pd.read_csv(filepath)\n",
    "\n",
    "# 상위 5개의 데이터 확인\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2817cda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 개수 확인\n",
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee8c3b",
   "metadata": {},
   "source": [
    "데이터가 그렇게 많지는 않으니, 전체 샘플을 사용하도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75a305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수 정의\n",
    "def preprocess_sentence(sentence):\n",
    "    # 영어는 소문자로 변환하고 양쪽 공백 제거\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # 마지막 단어와 !, ?, . 사이의 공백 추가\n",
    "    sentence = re.sub(r'([?.!,])', r' \\1 ', sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', ' ', sentence)\n",
    "    \n",
    "    # 한글, 영문, !, ?, ., ,를 제외한 문자는 공백으로 변환\n",
    "    sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z.?!,\\s]', ' ', sentence)\n",
    "    # 양쪽 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee45916",
   "metadata": {},
   "source": [
    "preprocess_sentence 함수의 동작 원리는 다음과 같다.\n",
    "* 양쪽 공백 제거\n",
    "> ex)[  눈살이 찌푸려지죠.  ] -> [눈살이 찌푸려지죠.]  \n",
    "* 마지막 단어와 구두점 사이의 공백 추가\n",
    "> ex)[눈살이 찌푸려지죠.] -> [눈살이 찌푸려지죠 .]  \n",
    "* 한글, 영문, 구두점과 쉼표를 제외한 문자는 공백으로 변환\n",
    "> ex) [눈살이 찌푸려지죠 .] -> [눈살이 찌푸려지죠 .]  \n",
    "* 공백 제거\n",
    "> ex) [눈살이 찌푸려지죠 .] -> [눈살이 찌푸려지죠.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d671b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: 12시 땡!\n",
      "Preprocessed Question: 시 땡 !\n",
      "Original Answer: 하루가 또 가네요.\n",
      "Preprocessed Answer: 하루가 또 가네요 .\n",
      "\n",
      "Original Question: 1지망 학교 떨어졌어\n",
      "Preprocessed Question: 지망 학교 떨어졌어\n",
      "Original Answer: 위로해 드립니다.\n",
      "Preprocessed Answer: 위로해 드립니다 .\n",
      "\n",
      "Original Question: 3박4일 놀러가고 싶다\n",
      "Preprocessed Question: 박 일 놀러가고 싶다\n",
      "Original Answer: 여행은 언제나 좋죠.\n",
      "Preprocessed Answer: 여행은 언제나 좋죠 .\n",
      "\n",
      "Original Question: 3박4일 정도 놀러가고 싶다\n",
      "Preprocessed Question: 박 일 정도 놀러가고 싶다\n",
      "Original Answer: 여행은 언제나 좋죠.\n",
      "Preprocessed Answer: 여행은 언제나 좋죠 .\n",
      "\n",
      "Original Question: PPL 심하네\n",
      "Preprocessed Question: ppl 심하네\n",
      "Original Answer: 눈살이 찌푸려지죠.\n",
      "Preprocessed Answer: 눈살이 찌푸려지죠 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 함수 적용\n",
    "preprocessed_questions = [preprocess_sentence(sentence) for sentence in raw_data['Q']]\n",
    "preprocessed_answers = [preprocess_sentence(sentence) for sentence in raw_data['A']]\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "for i in range(5):\n",
    "    print(f\"Original Question: {raw_data['Q'][i]}\")\n",
    "    print(f\"Preprocessed Question: {preprocessed_questions[i]}\")\n",
    "    print(f\"Original Answer: {raw_data['A'][i]}\")\n",
    "    print(f\"Preprocessed Answer: {preprocessed_answers[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3446e89b",
   "metadata": {},
   "source": [
    "정규표현식이 정상적으로 적용된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276dd864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수:  11823\n",
      "전체 샘플 수:  11823\n"
     ]
    }
   ],
   "source": [
    "# 전체 샘플 개수 확인\n",
    "print('전체 샘플 수: ', len(preprocessed_questions))\n",
    "print('전체 샘플 수: ', len(preprocessed_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b9c85",
   "metadata": {},
   "source": [
    "### STEP 4. SubwordTextEncoder 사용하기\n",
    "---\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분들이 알고 있다. 하지만 이 번 프로젝트에서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 `SubwordTextEncoder`를 그대로 사용해보도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36c7eb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(preprocessed_questions + preprocessed_answers, target_vocab_size=2**13)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd2ec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호:  [8146]\n",
      "END_TOKEN의 번호:  [8147]\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토근 지정\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# 생성된 시작토큰과 종료 토큰 확인\n",
    "print('START_TOKEN의 번호: ', [tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호: ', [tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "797b8c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8148\n"
     ]
    }
   ],
   "source": [
    "# 단어장 크기 지정\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "\n",
    "# 단어장 크기 확인\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b1992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5756, 613, 2495, 4160]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2361, 7495, 7, 6262, 98, 1]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩 후의 샘플 확인\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(preprocessed_questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(preprocessed_answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06f1b45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# 샘플 최대 길이 지정\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c67ba823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징 및 정수 인코딩 함수 정의\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "        \n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "            \n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    \n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91b9da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8148\n",
      "필터링 후의 질문 샘플 개수: 11823\n",
      "필터링 후의 답변 샘플 개수: 11823\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(preprocessed_questions, preprocessed_answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74112572",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    }\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae0ebc",
   "metadata": {},
   "source": [
    "훈련 프로세스의 속도가 빨라지도록 입력 파이프라인을 구축하여 **교사 강요** 훈련 기법을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50babbbe",
   "metadata": {},
   "source": [
    "### STEP 5. 모델 구성하기\n",
    "---\n",
    "앞 서 진행했던 실습 내용을 바탕으로 transformer 모델을 설계한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "678526a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어 구현\n",
    "class PositionalEncoding(Layer):\n",
    "    \n",
    "    # 생성자 정의\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "        \n",
    "    # Positional Encoding에서 사용할 각도 배열을 생성하는 메서드 정의\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "    \n",
    "    # Positional Encoding을 계산하는 메서드 정의\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 각 위치(position)과 차원(d_model)에 대한 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            position = tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model = d_model\n",
    "        )\n",
    "        \n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        # sin과 cosine이 교차하도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "        \n",
    "        # 배치 차원 추가\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        \n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    \n",
    "    # Positional Encoding을 입력에 더하여 반환하는 메서드\n",
    "    def call(self, inputs):\n",
    "        # 입력의 형태를 기반으로 Positional Encoding의 일부를 잘라내어 사용\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b60706",
   "metadata": {},
   "source": [
    "$$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$$  \n",
    "각 임베딩 벡터에 대해 해당 단어의 순서 정보를 더할 수 있도록, 위와 같은 연산을 수행하는 Positional Encoding함수를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00658420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PositioalEncoding 샘플 확인\n",
    "sample_pos_encoding = PositionalEncoding(40, 512)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13e21e",
   "metadata": {},
   "source": [
    "미리 설정한 최대 문장 길이인 40의 값과 512차원에 대한 샘플 positional encoding 값을 확인할 수 있다.  \n",
    "사실 위의 그래프에 대해서는 여러 자료를 찾아봤는데도 어떻게 해석을 해야할지 감이 안잡혀서 추후에 수식에 따른 시각화를 진행하여 비교분석을 해야할 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cc78eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 레이어 구현\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # query와 key의 행렬 곱셈 계산\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b = True)\n",
    "    # key의 마지막 차원 크기(d_k)를 float32 타입으로 변환\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    # 스케일링을 위해 matmul_qk를 depth의 제곱근으로 나눔\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "    \n",
    "    # mask가 비어있지 않을 때, masked softmax 계산을 위해 logits에 mask 값을 더함\n",
    "    if mask is not None:\n",
    "        # mask가 0인 위치에 매우 작은 값을 더해줌\n",
    "        logits += (mask * -1e9)\n",
    "        \n",
    "    # softmax 함수를 적용하여 attention weights 계산\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    # attention weights와 value의 행렬 곱셈을 통해 최종 output 계산\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70c24a",
   "metadata": {},
   "source": [
    "단어 간의 유사도(벡터 간의 내적값)를 연산하고 그 값을 정규화시키는 `scaled_dot_product_attention`함수를 정의하여 어텐션을 구현한다.  \n",
    "스케일드 닷 프로덕트 어텐션의 수식은 다음과 같다.  \n",
    "$$Attention(Q, K, V) = softmax({{QK^T}\\over {\\sqrt {d_k}}})V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cb02d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 헤드 어텐션 레이어 구현\n",
    "class MultiHeadAttention(Layer):\n",
    "    # 생성자 메서드 정의\n",
    "    def __init__(self, d_model, num_heads, name='multi_head_attention'):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # d_model이 num_heads로 나누어 떨어지는지 확인\n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        # 각 헤드의 차원 계산\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # query, key, value에 대한 Dense 레이어 생성\n",
    "        self.query_dense = Dense(units=d_model)\n",
    "        self.key_dense = Dense(units=d_model)\n",
    "        self.value_dense = Dense(units=d_model)\n",
    "        # 마지막에 연결하는 Dense 레이어 생성\n",
    "        self.dense = Dense(units=d_model)\n",
    "        \n",
    "    # 입력을 여러 헤드로 나누어 형상을 변환하는 메서드 정의\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        \n",
    "        # 형상 (batch_size, num_heads, seq_len, depth)\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    # 멀티 헤드 어텐션 수행 메서드 정의\n",
    "    def call(self, inputs):\n",
    "        # inputs의 각 텐서 정보 저장\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        # 배치 사이즈 지정\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # query, key, value에 대한 Dense 레이어 적용\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "        \n",
    "        # 지정된 헤드 개수로 분할\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        \n",
    "        # Scaled Dot-Product Attention 수행\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        \n",
    "        # 헤드 결합\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        # 최종 Dense 레이어 적용\n",
    "        outputs = self.dense(concat_attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b82c60",
   "metadata": {},
   "source": [
    "지정된 head의 개수만큼 병렬로 수행되는 셀프 어텐션 결과를 얻을 수 있는 `MultiHeadAttention`함수를 구현한다. 멀티 헤드 어텐션은 각각의 head가 서로 다른 관점에서 셀프 어텐션을 수행하기 때문에 한 번의 어텐션만 수행한다면 자칫 놓질 수도 있는 정보를 캐치할 수 있다.  \n",
    "> ex) 12시 땡! 하면 못 푼 문제는 땡으로 간주할거야!라는 문장에서 '12시'라는 토큰이 정각을 의미하는 '땡'과 유사하다고 보는 관점과 틀림을 의미하는 '땡'과 유사하다고 보는 관점을 한 번에 표현할 수 있다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53abf7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 마스크 생성 함수 정의\n",
    "def create_padding_mask(x):\n",
    "    # x가 0이면 패딩 마스크로 설정\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    \n",
    "    # 패딩 마스크의 형태를 (batch_size, 1, 1, seq_len)으로 변환\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d542b",
   "metadata": {},
   "source": [
    "||||\n",
    "|:----:|:----:|:----:|\n",
    "|나는|최고야.|0|\n",
    "|나는|진심으로|최고야.|\n",
    "|나는|진심으로|최고가|  \n",
    "\n",
    "위의 표와 같이 지정된 최대 길이보다 짧은 문장은 빈 공간을 0으로 채우고, 긴 문장은 자르는 자연어 전처리 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5a5f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 룩어헤드 마스크 생성 함수 정의\n",
    "def create_look_ahead_mask(x):\n",
    "    # 입력 텐서 x의 형상에서 시퀀스의 길이 추출\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    # 1로 채워진 행렬을 생성하고 트라이얼 매트릭스로 변환\n",
    "    # 현재 위치 이후의 위치에 대해서는 0으로, 이전의 위치에 대해서는 1로 채워, 현재 위치에서 이후의 위치를 볼 수 있도록 한다.\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    # 입력 데이터 x에서 패딩된 부분을 확인하는 패딩 마스크 생성\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    \n",
    "    # 룩어헤드 마스크와 패딩 마스크를 합친 다음,\n",
    "    # 두 마스크 중 하나라도 1인 위치는 1로 설정되어 최종 마스크를 생성\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d412e8",
   "metadata": {},
   "source": [
    "트랜스포머의 경우, 전체 문장이 문장 행렬로 들어가기 때문에 위치와 상관없이 모든 단어를 참고해서 다음 단어를 예측할 수 있다. 하지만 사실 우리가 원하는 것은 이전 단어들로부터 다음 단어를 예측하는 훈련을 제대로 하는 것이기 때문에 이러한 문제를 해결하기 위해 자신보다 다음에 나올 단어를 참고하지 않도록 가리는 작업을 해줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c086d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 레이어 함수 정의\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name='encoder_layer'):\n",
    "    # 입력 시퀀스의 모양을 정의하는 Input 레이어 생성\n",
    "    inputs = Input(shape=(None, d_model), name='inputs')\n",
    "    # 패딩 마스크를 입력으로 받는 Input 레이어 생성\n",
    "    padding_mask = Input(shape=(1,1,None), name='padding_mask')\n",
    "    \n",
    "    # 멀티헤드 어텐션 레이어 생성\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name='attention')({\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': padding_mask\n",
    "    })\n",
    "    \n",
    "    # 드롭아웃과 레이어 정규화를 적용하여 모델의 일반화 및 학습 안정성 향상\n",
    "    attention = Dropout(rate=dropout)(attention)\n",
    "    attention = LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "    \n",
    "    # 피드포워드 신경망의 첫 번째 밀집 레이어 생성\n",
    "    outputs = Dense(units=units, activation='relu')(attention)\n",
    "    # 피드포워드 신경망의 두 번째 밀집 레이어 생성\n",
    "    outputs = Dense(units=d_model)(outputs)\n",
    "    \n",
    "    # 드롭아웃과 레이어 정규화 적용\n",
    "    outputs = Dropout(rate=dropout)(outputs)\n",
    "    outputs = LayerNormalization(epsilon=1e-6)(attention+outputs)\n",
    "    \n",
    "    # 인코더 레이어 모델 정의 및 반환\n",
    "    return Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b9905",
   "metadata": {},
   "source": [
    "위에서 생성한 각 레이어들을 병합하여 트랜스포머의 인코더 층을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "479f0aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 구조 설계 함수 정의\n",
    "def encoder(vocab_size,\n",
    "           num_layers,\n",
    "           units,\n",
    "           d_model,\n",
    "           num_heads,\n",
    "           dropout,\n",
    "           name='encoder'):\n",
    "    # 입력 시퀀스의 모양을 정의하는 Input 레이어 생성\n",
    "    inputs = Input(shape=(None,), name='inputs')\n",
    "    # 패딩 마스크를 입력으로 받는 Input 레이어 생성\n",
    "    padding_mask = Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    # 임베딩 레이어 생성 후, 입력 토큰을 임베딩 벡터로 변환\n",
    "    embeddings = Embedding(vocab_size, d_model)(inputs)\n",
    "    # 임베딩 벡터에 대한 스케일 조정 수행\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    \n",
    "    # 포지셔널 인코딩 추가\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    # 드롭아웃을 적용하여 모델의 일반화 및 학습 안정성 향상\n",
    "    outputs = Dropout(rate=dropout)(embeddings)\n",
    "    \n",
    "    # 인코더 레이어를 여러 번 쌓음\n",
    "    for i in range(num_layers):\n",
    "        # 각각의 인코더 레이어에 입력 데이터와 패딩 마스크 전달\n",
    "        outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='encoder_layer_{}'.format(i),\n",
    "        )([outputs, padding_mask])\n",
    "        \n",
    "    # 인코더 모델 정의 및 반환\n",
    "    return Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0159e0",
   "metadata": {},
   "source": [
    "트랜스포머 모델의 인코터 레이어의 구조와 같이 '입력 -> 임베딩 -> 포지셔널 인코딩 -> 여러 개의 인코더 레이어' 구조가 쌓일 수 있도록 인코더 함수 구조를 설계한다.  \n",
    "인코더의 내부에는 각 서브 층 이후에 훈련을 돕는 **Layer Normalization**이라는 기법이 사용되었고, **num_layers**개수 만큼의 인코더 층을 쌓았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9985c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 레이어 함수 정의\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name='decoder_layer'):\n",
    "    # 입력 시퀀스의 모양을 정의하는 Input 레이어 생성\n",
    "    inputs = Input(shape=(None, d_model), name='inputs')\n",
    "    # 안코더의 출력을 입력으로 받는 Input 레이어 생성\n",
    "    enc_outputs = Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    # 룩어헤드 마스크를 입력으로 받는 Input 레이어 생성\n",
    "    look_ahead_mask = Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "    # 패딩 마스크를 입력으로 받는 Input 레이어 생성\n",
    "    padding_mask = Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    # 첫 번째 멀티헤드 어텐션: 셀프 어텐션\n",
    "    attention1 = MultiHeadAttention(d_model, num_heads, name='attention_1')(inputs={\n",
    "        'query': inputs,\n",
    "        'key': inputs,\n",
    "        'value': inputs,\n",
    "        'mask': look_ahead_mask\n",
    "    })\n",
    "    # 첫 번째 어텐션 후 레이어 정규화 및 잔차 연결\n",
    "    attention1 = LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "    \n",
    "    # 두 번째 멀티헤드 어텐션: 인코더-디코더 어텐션\n",
    "    attention2 = MultiHeadAttention(d_model, num_heads, name='attention_2')(inputs={\n",
    "        'query': attention1,\n",
    "        'key': enc_outputs,\n",
    "        'value': enc_outputs,\n",
    "        'mask': padding_mask\n",
    "    })\n",
    "    # 드롭아웃 적용 및 레이어 정규화 및 잔차 연결\n",
    "    attention2 = Dropout(rate=dropout)(attention2)\n",
    "    attention2 = LayerNormalization(epsilon=1e-6)(attention2 + attention 1)\n",
    "    \n",
    "    # 피드포워드 신경망의 첫 번째 밀집 레이어 생성\n",
    "    outputs = Dense(units=units, activation='relu')(attention2)\n",
    "    # 피드포워드 신경망의 두 번째 밀집 레이어 생성\n",
    "    outputs = Dense(units=d_model)(outputs)\n",
    "    \n",
    "    # 드롭아웃 적용 및 레이어 정규화 및 잔차 연결\n",
    "    outputs = Dropout(rate=dropout)(outputs)\n",
    "    outputs = LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "    \n",
    "    # 디코더 레이어 모델 정의 및 반환\n",
    "    return Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d7cb9",
   "metadata": {},
   "source": [
    "디코더는 인코더와 비슷하지만, 인코더보다 조금 더 복잡하다. 인코더는 두 개의 서브 층으로 구성되어 있는 반면에 디코더는 세 개의 서브 층으로 구성이 되어있다.  \n",
    "추가된 서브층은 **인코더-디코더 어텐션**으로, 셀프 어텐션과는 달리, Query가 디코더의 벡터인 반면에 Key와 Value가 인코더의 벡터라는 특징이 있다. 이 부분이 인코더가 입력 문장으로부터 정보를 디코더에 전달하는 부분이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26c3f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 구조 설계 함수 정의\n",
    "def decoder(vocab_size,\n",
    "           num_layers,\n",
    "           units,\n",
    "           d_model,\n",
    "           num_heads,\n",
    "           dropout,\n",
    "           name='decoder'):\n",
    "    # 입력 시퀀스의 모양을 정의하는 Input 레이어 생성\n",
    "    inputs = Input(shape=(None,), name='inputs')\n",
    "    # 인코더의 출력을 입력으로 받는 Input 레이어 생성\n",
    "    enc_outputs = Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    # 디코더의 셀프 어텐션에서 사용할 룩어헤드 마스크를 입력으로 받는 Input 레이어 생성\n",
    "    look_ahead_mask = Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "    # 패딩 마스크를 입력으로 받는 Input 레이어 생성\n",
    "    padding_mask = Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    # 임베딩 레이어 생성 후, 입력 토큰을 임베딩 벡터로 변환\n",
    "    embeddings = Embedding(vocab_size, d_model)(inputs)\n",
    "    # 임베딩 벡터에 대한 스케일 조정 수행\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    # 포지셔널 인코딩 추가\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    \n",
    "    # 드롭아웃을 적용하여 모델의 일반화 및 학습 안정성 향상\n",
    "    outputs = Dropout(rate=dropout)(embeddings)\n",
    "    \n",
    "    # 디코더 레이어를 여러 번 쌓음\n",
    "    for i in range(num_layers):\n",
    "        # 각각의 디코더 레이어에 입력 데이터와 마스크 전달\n",
    "        outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i))(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "        \n",
    "    # 전체 디코더 모델을 정의하고 반환\n",
    "    return Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7e3ae",
   "metadata": {},
   "source": [
    "먼저 설계한 인코더와 같이 임베딩 층과 포지셔널 인코딩층을 연결하고, 여러 개의 디코더 층을 쌓아 트랜스포머의 디코더를 설계했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65291e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 구조 설계 함수 정의\n",
    "def transformer(vocab_size,\n",
    "               num_layers,\n",
    "               units,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dropout,\n",
    "               name='transformer'):\n",
    "    # 입력 및 디코더 입력의 모양을 정의하는 Input 레이어 생성\n",
    "    inputs = Input(shape=(None,), name='inputs')\n",
    "    dec_inputs = Input(shape=(None,), name='dec_inputs')\n",
    "    \n",
    "    # 인코더의 패딩 마스크를 생성하는 Lambda 레이어 생성\n",
    "    enc_padding_mask = Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name = 'enc_padding_mask')(inputs)\n",
    "    # 디코더의 룩어헤드 마스크를 생성하는 Lambda 레이어 생성\n",
    "    look_ahead_mask = Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape = (1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "    # 디코더의 패딩 마스크를 생성하는 Lambda 레이어 생성\n",
    "    dec_padding_mask = Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "    \n",
    "    # 인코더 레이어를 호출하여 인코더의 출력을 얻고, 인코더 입력과 패딩 마스크를 전달\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size = vocab_size,\n",
    "        num_layers = num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "    # 디코더 레이어를 호출하여 디코더의 출력을 얻고, 디코더 입력, 인코더 출력, 룩어헤드 마스크, 패딩 마스크를 전달\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "    # 출력 레이어 정의. 출력은 단어 집합 크기와 동일한 차원을 가지며, 각 위치의 값을 통해 다음 단어 예측\n",
    "    outputs = Dense(units=vocab_size, name='outputs')(dec_outputs)\n",
    "    \n",
    "    # 전체 트랜스포머 모델을 정의하고 반환\n",
    "    return Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1891b1",
   "metadata": {},
   "source": [
    "앞 서 설계한 인코더와 디코더를 연결하여 트랜스포머 구조를 설계했다. 트랜스포머의 생성과정은 다음과 같다.  \n",
    "![transformer](./transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c10f437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3140096     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3667456     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8148)   2094036     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,901,588\n",
      "Trainable params: 8,901,588\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로우 세션 초기화\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 전역변수 설정\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# 트랜스포머 모델 생성\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "# 모델 구조 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddcfa9a",
   "metadata": {},
   "source": [
    "설계한대로 모델의 구조와 파라미터, 형상 등이 잘 생성된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13723e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 함수 정의\n",
    "def loss_function(y_true, y_pred):\n",
    "    # y_true의 형태를 변경하여 시퀀스 끝 토큰 제거\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    \n",
    "    # SparseCategoricalCrossentropy 손실 계산\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred)\n",
    "    \n",
    "    # 손실 계산에서 패딩 토큰을 무시하기 위한 마스크 생성\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    # 마스크를 손실에 적용\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    \n",
    "    # 패딩 토큰을 제외한 토큰들에 대한 평균 손실을 계산하여 반환\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b9c79",
   "metadata": {},
   "source": [
    "레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b567c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 학습률 스케쥴링 클래스 정의\n",
    "class CustomSchedule(LearningRateSchedule):\n",
    "    # 클래스의 생성자 정의\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        # 모델의 임베딩 차원 저장\n",
    "        self.d_model = d_model\n",
    "        # 모델의 임베딩 차원을 실수형으로 저장\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        \n",
    "        # 웜업 스텝 수를 저장\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    # 객체가 호출될 때, 현재 스텝에 대한 학습률을 계산하는 메소드 정의\n",
    "    def __call__(self, step):\n",
    "        # 현재 스텝의 제곱근의 역수 계산\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        # 웜업 스텝 수에 따라 선형적으로 증가하는 두 번째 항 계산\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        # 모델의 임베딩 차원에 제곱근의 역수를 곱하고, 두 계산된 항 중 작은 값을 선택하여 반환\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c41ed7",
   "metadata": {},
   "source": [
    "모델학습 초기에 learning rate를 급격히 높였다가, 이후 train step이 진행됨에 따라 서서히 낮추어 가면서 안정적으로 수렴하게 하는 커스텀 학습률 스케쥴링을 적용한다.  \n",
    "계산 공식은 다음과 같다.  \n",
    "$$lrate = {d_{model}}^{-0.5} \\cdot min({stepNum}^{-0.5},{stepNum}\\cdot {warmupSteps^{-0.5}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df372e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률 설정\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# 정확도 함수 정의\n",
    "def accuracy(y_true, y_pred):\n",
    "    # y_true의 형태를 변경하여 시퀀스 끝 토큰 제거\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    \n",
    "    # 평가지표 반환\n",
    "    return sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b47450",
   "metadata": {},
   "source": [
    "지정한 평가지표와 옵티마이저, 학습률 등으로 모델을 컴파일 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99dc1c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "185/185 [==============================] - 16s 53ms/step - loss: 1.4519 - accuracy: 0.0280\n",
      "Epoch 2/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 1.1676 - accuracy: 0.0496\n",
      "Epoch 3/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.9940 - accuracy: 0.0509\n",
      "Epoch 4/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.9198 - accuracy: 0.0551\n",
      "Epoch 5/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.8584 - accuracy: 0.0584\n",
      "Epoch 6/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.7944 - accuracy: 0.0631\n",
      "Epoch 7/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.7239 - accuracy: 0.0702\n",
      "Epoch 8/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.6446 - accuracy: 0.0790\n",
      "Epoch 9/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.5573 - accuracy: 0.0891\n",
      "Epoch 10/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.4686 - accuracy: 0.0998\n",
      "Epoch 11/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.3799 - accuracy: 0.1117\n",
      "Epoch 12/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.2964 - accuracy: 0.1239\n",
      "Epoch 13/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.2220 - accuracy: 0.1352\n",
      "Epoch 14/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.1586 - accuracy: 0.1459\n",
      "Epoch 15/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.1101 - accuracy: 0.1547\n",
      "Epoch 16/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0758 - accuracy: 0.1609\n",
      "Epoch 17/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0542 - accuracy: 0.1645\n",
      "Epoch 18/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0435 - accuracy: 0.1660\n",
      "Epoch 19/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0370 - accuracy: 0.1671\n",
      "Epoch 20/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0349 - accuracy: 0.1670\n",
      "Epoch 21/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0326 - accuracy: 0.1675\n",
      "Epoch 22/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0319 - accuracy: 0.1675\n",
      "Epoch 23/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0289 - accuracy: 0.1681\n",
      "Epoch 24/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0246 - accuracy: 0.1692\n",
      "Epoch 25/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0214 - accuracy: 0.1699\n",
      "Epoch 26/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0193 - accuracy: 0.1705\n",
      "Epoch 27/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0174 - accuracy: 0.1709\n",
      "Epoch 28/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0150 - accuracy: 0.1716\n",
      "Epoch 29/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0139 - accuracy: 0.1718\n",
      "Epoch 30/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0129 - accuracy: 0.1720\n",
      "Epoch 31/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0115 - accuracy: 0.1724\n",
      "Epoch 32/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0108 - accuracy: 0.1726\n",
      "Epoch 33/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0096 - accuracy: 0.1728\n",
      "Epoch 34/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0093 - accuracy: 0.1729\n",
      "Epoch 35/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0085 - accuracy: 0.1731\n",
      "Epoch 36/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0081 - accuracy: 0.1732\n",
      "Epoch 37/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0074 - accuracy: 0.1733\n",
      "Epoch 38/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0072 - accuracy: 0.1735\n",
      "Epoch 39/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0070 - accuracy: 0.1735\n",
      "Epoch 40/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0066 - accuracy: 0.1735\n",
      "Epoch 41/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0059 - accuracy: 0.1737\n",
      "Epoch 42/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0057 - accuracy: 0.1738\n",
      "Epoch 43/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0056 - accuracy: 0.1738\n",
      "Epoch 44/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0053 - accuracy: 0.1739\n",
      "Epoch 45/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0055 - accuracy: 0.1739\n",
      "Epoch 46/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0047 - accuracy: 0.1740\n",
      "Epoch 47/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0046 - accuracy: 0.1740\n",
      "Epoch 48/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0046 - accuracy: 0.1739\n",
      "Epoch 49/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0045 - accuracy: 0.1740\n",
      "Epoch 50/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0042 - accuracy: 0.1741\n",
      "Epoch 51/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0041 - accuracy: 0.1741\n",
      "Epoch 52/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0038 - accuracy: 0.1741\n",
      "Epoch 53/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0038 - accuracy: 0.1742\n",
      "Epoch 54/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0035 - accuracy: 0.1742\n",
      "Epoch 55/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0037 - accuracy: 0.1742\n",
      "Epoch 56/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0034 - accuracy: 0.1742\n",
      "Epoch 57/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0034 - accuracy: 0.1743\n",
      "Epoch 58/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0032 - accuracy: 0.1743\n",
      "Epoch 59/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0032 - accuracy: 0.1743\n",
      "Epoch 60/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0031 - accuracy: 0.1742\n",
      "Epoch 61/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0031 - accuracy: 0.1743\n",
      "Epoch 62/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0028 - accuracy: 0.1743\n",
      "Epoch 63/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0028 - accuracy: 0.1743\n",
      "Epoch 64/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0030 - accuracy: 0.1743\n",
      "Epoch 65/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0026 - accuracy: 0.1743\n",
      "Epoch 66/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0027 - accuracy: 0.1743\n",
      "Epoch 67/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0027 - accuracy: 0.1744\n",
      "Epoch 68/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0025 - accuracy: 0.1744\n",
      "Epoch 69/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0025 - accuracy: 0.1744\n",
      "Epoch 70/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0024 - accuracy: 0.1744\n",
      "Epoch 71/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0023 - accuracy: 0.1744\n",
      "Epoch 72/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0022 - accuracy: 0.1744\n",
      "Epoch 73/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0025 - accuracy: 0.1743\n",
      "Epoch 74/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0020 - accuracy: 0.1744\n",
      "Epoch 75/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0021 - accuracy: 0.1744\n",
      "Epoch 76/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0021 - accuracy: 0.1744\n",
      "Epoch 77/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0019 - accuracy: 0.1744\n",
      "Epoch 78/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0019 - accuracy: 0.1745\n",
      "Epoch 79/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0020 - accuracy: 0.1744\n",
      "Epoch 80/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0020 - accuracy: 0.1744\n",
      "Epoch 81/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0021 - accuracy: 0.1744\n",
      "Epoch 82/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0020 - accuracy: 0.1744\n",
      "Epoch 83/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0019 - accuracy: 0.1744\n",
      "Epoch 84/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0018 - accuracy: 0.1745\n",
      "Epoch 85/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0018 - accuracy: 0.1744\n",
      "Epoch 86/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0017 - accuracy: 0.1745\n",
      "Epoch 87/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0018 - accuracy: 0.1744\n",
      "Epoch 88/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1745\n",
      "Epoch 89/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0017 - accuracy: 0.1745\n",
      "Epoch 90/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0017 - accuracy: 0.1744\n",
      "Epoch 91/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0016 - accuracy: 0.1745\n",
      "Epoch 92/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1745\n",
      "Epoch 93/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1745\n",
      "Epoch 94/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0016 - accuracy: 0.1744\n",
      "Epoch 95/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0017 - accuracy: 0.1745\n",
      "Epoch 96/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1744\n",
      "Epoch 97/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1745\n",
      "Epoch 98/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1744\n",
      "Epoch 99/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1745\n",
      "Epoch 100/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1745\n",
      "Epoch 101/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1745\n",
      "Epoch 102/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1745\n",
      "Epoch 103/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1745\n",
      "Epoch 104/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1745\n",
      "Epoch 105/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1745\n",
      "Epoch 106/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 107/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1745\n",
      "Epoch 108/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1745\n",
      "Epoch 109/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1745\n",
      "Epoch 110/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1745\n",
      "Epoch 111/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 112/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1745\n",
      "Epoch 113/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1745\n",
      "Epoch 114/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1745\n",
      "Epoch 115/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 116/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 117/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 118/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 119/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 120/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1745\n",
      "Epoch 121/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 122/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 123/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 124/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1745\n",
      "Epoch 125/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1745\n",
      "Epoch 126/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 127/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1745\n",
      "Epoch 128/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 129/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1745\n",
      "Epoch 130/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1745\n",
      "Epoch 131/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1745\n",
      "Epoch 132/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 133/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1745\n",
      "Epoch 134/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 135/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1745\n",
      "Epoch 136/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.6159e-04 - accuracy: 0.1745\n",
      "Epoch 137/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1745\n",
      "Epoch 138/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.8407e-04 - accuracy: 0.1745\n",
      "Epoch 139/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.9892e-04 - accuracy: 0.1745\n",
      "Epoch 140/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4729e-04 - accuracy: 0.1745\n",
      "Epoch 141/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1745\n",
      "Epoch 142/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.8361e-04 - accuracy: 0.1745\n",
      "Epoch 143/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1746\n",
      "Epoch 144/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.9522e-04 - accuracy: 0.1745\n",
      "Epoch 145/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.7630e-04 - accuracy: 0.1745\n",
      "Epoch 146/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.5475e-04 - accuracy: 0.1745\n",
      "Epoch 147/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1745\n",
      "Epoch 148/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.2762e-04 - accuracy: 0.1745\n",
      "Epoch 149/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1745\n",
      "Epoch 150/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1745\n",
      "Epoch 151/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4053e-04 - accuracy: 0.1745\n",
      "Epoch 152/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.6017e-04 - accuracy: 0.1745\n",
      "Epoch 153/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.0233e-04 - accuracy: 0.1745\n",
      "Epoch 154/200\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.0010 - accuracy: 0.1745\n",
      "Epoch 155/200\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 9.3600e-04 - accuracy: 0.1745\n",
      "Epoch 156/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.2022e-04 - accuracy: 0.1745\n",
      "Epoch 157/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.8607e-04 - accuracy: 0.1745\n",
      "Epoch 158/200\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 8.9357e-04 - accuracy: 0.1745\n",
      "Epoch 159/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.8085e-04 - accuracy: 0.1745\n",
      "Epoch 160/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.7594e-04 - accuracy: 0.1745\n",
      "Epoch 161/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4944e-04 - accuracy: 0.1745\n",
      "Epoch 162/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4888e-04 - accuracy: 0.1745\n",
      "Epoch 163/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1745\n",
      "Epoch 164/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.0245e-04 - accuracy: 0.1745\n",
      "Epoch 165/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.9093e-04 - accuracy: 0.1745\n",
      "Epoch 166/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.6543e-04 - accuracy: 0.1745\n",
      "Epoch 167/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.1389e-04 - accuracy: 0.1745\n",
      "Epoch 168/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.8487e-04 - accuracy: 0.1745\n",
      "Epoch 169/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.5202e-04 - accuracy: 0.1745\n",
      "Epoch 170/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.0874e-04 - accuracy: 0.1745\n",
      "Epoch 171/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.5131e-04 - accuracy: 0.1745\n",
      "Epoch 172/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3657e-04 - accuracy: 0.1745\n",
      "Epoch 173/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.3924e-04 - accuracy: 0.1745\n",
      "Epoch 174/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.8903e-04 - accuracy: 0.1745\n",
      "Epoch 175/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.8936e-04 - accuracy: 0.1746\n",
      "Epoch 176/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.2103e-04 - accuracy: 0.1745\n",
      "Epoch 177/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.6557e-04 - accuracy: 0.1745\n",
      "Epoch 178/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.2883e-04 - accuracy: 0.1745\n",
      "Epoch 179/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9928e-04 - accuracy: 0.1745\n",
      "Epoch 180/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.5566e-04 - accuracy: 0.1745\n",
      "Epoch 181/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.5957e-04 - accuracy: 0.1745\n",
      "Epoch 182/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9219e-04 - accuracy: 0.1745\n",
      "Epoch 183/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9554e-04 - accuracy: 0.1745\n",
      "Epoch 184/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9572e-04 - accuracy: 0.1745\n",
      "Epoch 185/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.7630e-04 - accuracy: 0.1745\n",
      "Epoch 186/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4905e-04 - accuracy: 0.1745\n",
      "Epoch 187/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.6130e-04 - accuracy: 0.1745\n",
      "Epoch 188/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.1572e-04 - accuracy: 0.1745\n",
      "Epoch 189/200\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 8.7686e-04 - accuracy: 0.1745\n",
      "Epoch 190/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.1854e-04 - accuracy: 0.1745\n",
      "Epoch 191/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.4290e-04 - accuracy: 0.1745\n",
      "Epoch 192/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3502e-04 - accuracy: 0.1745\n",
      "Epoch 193/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7825e-04 - accuracy: 0.1745\n",
      "Epoch 194/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.6108e-04 - accuracy: 0.1745\n",
      "Epoch 195/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.1627e-04 - accuracy: 0.1745\n",
      "Epoch 196/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0080e-04 - accuracy: 0.1745\n",
      "Epoch 197/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.1444e-04 - accuracy: 0.1746\n",
      "Epoch 198/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6448e-04 - accuracy: 0.1745\n",
      "Epoch 199/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.1348e-04 - accuracy: 0.1745\n",
      "Epoch 200/200\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0100e-04 - accuracy: 0.1745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2ddb311c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 에폭 설정\n",
    "EPOCHS = 200\n",
    "# 모델 학습 시작\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375d410",
   "metadata": {},
   "source": [
    "### STEP 4. 모델 평가하기\n",
    "---\n",
    "STEP 2에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f5273ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f2c8b",
   "metadata": {},
   "source": [
    "위의 함수가 수행하는 예측 단계는 기본적으로 다음과 같은 단계를 거친다.  \n",
    "* 새로운 입력 문장에 대해서는 훈련 때와 동일한 전처리를 거친다.\n",
    "* 입력 문장을 토크나이징하고, START_TOKEN과 END_TOKEN을 추가한다.\n",
    "* 패딩 마스킹과 룩 어헤드 마스킹을 계산한다.\n",
    "* 디코더는 입력 시퀀스로부터 다음 단어를 예측한다.\n",
    "* 디코더는 예측된 다음 단어를 기존의 입력 시퀀스에 추가하여 새로운 입력으로 사용한다.\n",
    "* END_TOKEN이 예측되거나 문장의 최대 길이에 도달하면 디코더는 동작을 멈춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a66bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode([i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3bf4f3",
   "metadata": {},
   "source": [
    "입력 문장에 대해 한국어로 답변하는 함수를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71fc1578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 방학이 언제야?\n",
      "출력 : 학생이 아니니 어쩔 수 없죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'학생이 아니니 어쩔 수 없죠 .'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('방학이 언제야?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18d6d50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 넌 똑똑하니?\n",
      "출력 : 저는 위로봇입니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저는 위로봇입니다 .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('넌 똑똑하니?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96dcb10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 나는 지금 정말 피곤해.\n",
      "출력 : 정신 노동을 했나 봐요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'정신 노동을 했나 봐요 .'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('나는 지금 정말 피곤해.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "666723b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 너를 만드는게 정신 노동인가? 그렇다면 난 행복한 사람이야.\n",
      "출력 : 원망과 슬픔에 잠기는 게 아니라 그 시간들을 가만히 내려놓고 내가 어떤 사람이었는가를 돌아보는 것입니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'원망과 슬픔에 잠기는 게 아니라 그 시간들을 가만히 내려놓고 내가 어떤 사람이었는가를 돌아보는 것입니다 .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('너를 만드는게 정신 노동인가? 그렇다면 난 행복한 사람이야.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5c79a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 너는 무슨 챗봇이니?\n",
      "출력 : 저는 사람으로 태어나고 싶어요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저는 사람으로 태어나고 싶어요 .'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('너는 무슨 챗봇이니?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52f62a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 그건 불가능해.\n",
      "출력 : 지금도 그래요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'지금도 그래요 .'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('그건 불가능해.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9865ec69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 무슨 말을 하는거야?\n",
      "출력 : 솔직한 마음으로 다가가는 거죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'솔직한 마음으로 다가가는 거죠 .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('무슨 말을 하는거야?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd965d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 누구한테?\n",
      "출력 : 저도 궁금하네요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저도 궁금하네요 .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('누구한테?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "042c327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 너도 모르는 답변을 하면 어떡해?\n",
      "출력 : 그게 최고죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그게 최고죠 .'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('너도 모르는 답변을 하면 어떡해?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8265373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 너 성격이 굉장히 막무가내구나?\n",
      "출력 : 평소 흘려 이야기했던 걸 기억해주고 생각하고 있다는 걸 알려주세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'평소 흘려 이야기했던 걸 기억해주고 생각하고 있다는 걸 알려주세요 .'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('너 성격이 굉장히 막무가내구나?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa2d4e",
   "metadata": {},
   "source": [
    "재밌다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867f2f1",
   "metadata": {},
   "source": [
    "## SUMMARY\n",
    "---\n",
    "한국어 데이터셋과 트랜스포머 모델을 이용하여 한국어 대화형 챗봇을 만들어봤다. 대부분은 얼토당토않는 얘기를 하지만, 종종 그럴듯한 답변을 내놓는 모델이 생성되었다. 트랜스포머의 구조에 대해 아직 완벽히 이해를 하지 못 한만큼 학습을 더 진행하고 다시 한 번 도전하면 더욱 발전된 챗봇을 만들 수 있을거라는 생각이 든다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "## 회고\n",
    "---\n",
    "### KEEP\n",
    "* 최선을 다해 이해하기 위해서 많은 강의를 찾아봤다.  \n",
    "\n",
    "### PROBLEM\n",
    "* 이해하기가 너무 어려운 구조이다.\n",
    "* 코드 자체에 대한 이해도 못 끝냈다.\n",
    "* chatGPT에 대한 의존도가 높았다.  \n",
    "\n",
    "### TRY\n",
    "* 5번 이상 필사를 해보고 코드 한 줄 한 줄에 대한 주석을 이해한다.\n",
    "* 간단한 설명부터 심화된 설명의 강의 영상을 찾아본다.\n",
    "* 한 번 의존했으니, 6번째 도전에서는 GPT의 도움 없이 진행해본다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
